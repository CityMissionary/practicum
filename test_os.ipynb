{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\data\\CA\\en-US\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ndjson\n",
    "import fnmatch\n",
    "\n",
    "file_list=os.listdir('E:\\\\data')\n",
    "# file_list\n",
    "\n",
    "path=[]\n",
    "for i in file_list:\n",
    "    path.append('E:\\\\data\\\\'+os.path.join(i)+'\\\\en-US')\n",
    "print(path[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\vscode\\code\\practicum\\outcome\\output_BR.csv\n",
      "C:\\vscode\\code\\practicum\\outcome\\output_CA.csv\n",
      "C:\\vscode\\code\\practicum\\outcome\\output_CN.csv\n"
     ]
    }
   ],
   "source": [
    "type(path[1])\n",
    "for i in file_list:\n",
    "    print(\"C:\\\\vscode\\\\code\\\\practicum\\\\outcome\\\\output_\"+i+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22780\\3690405428.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfnmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfnmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SP_FILING_REFERENCE*'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf_8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m                 \u001b[0mref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mndjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mcombined_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\19427\\.conda\\envs\\practicum\\lib\\encodings\\__init__.py\u001b[0m in \u001b[0;36msearch_function\u001b[1;34m(encoding)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0msearch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;31m# Cache lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i in file_list:\n",
    "    # folder with the Reference, Data, Word Count, and Sentiment zipped files\n",
    "    # os.chdir('C:\\\\vscode\\\\code\\\\practicum\\\\JSON_Data')\n",
    "    os.chdir('E:\\\\data\\\\'+os.path.join(i)+'\\\\en-US')\n",
    "\n",
    "    flag=0\n",
    "    #### Filing Reference Package ####\n",
    "    # create an empty list, combined_ref, and loop through each file and combine to combined_ref\n",
    "    # combined ref will be a list of dictionaries\n",
    "    combined_ref = []\n",
    "    for file in os.listdir():\n",
    "        if(fnmatch.fnmatch(file, 'SP_FILING_REFERENCE*')):\n",
    "            with open(file,'r',encoding='utf_8') as ref:\n",
    "                ref = ndjson.load(ref)\n",
    "            combined_ref.extend(ref)    \n",
    "    \n",
    "    # normalize the reference package from its NDJSON form into a Data Frame - uses from pandas.io.json import json_normalize\n",
    "    ref_normalize = pd.json_normalize(combined_ref)\n",
    "\n",
    "    # change order of columns to reflect User Guide documentation\n",
    "    ref_normalize = ref_normalize[['ID', 'DOCUMENT_ID','DOCUMENT_TYPE','FILING_DATE','MODIFIED_AT','DETAIL_JSON.company_name', 'DETAIL_JSON.parsing_status']]\n",
    "\n",
    "    # convert the ID and document ID field to a string\n",
    "    ref_normalize['ID'] = ref_normalize['ID'].apply(str)\n",
    "    ref_normalize['DOCUMENT_ID'] = ref_normalize['DOCUMENT_ID'].apply(str)\n",
    "    # ref_normalize['DETAIL_JSON.ISIN_active'] = ref_normalize['DETAIL_JSON.ISIN_active'].apply(str)\n",
    "    # ref_normalize['DETAIL_JSON.SP_DocumentId'] = ref_normalize['DETAIL_JSON.SP_DocumentId'].apply(str)\n",
    "\n",
    "    #### Sentiment Score Package ####\n",
    "    # create an empty list, combined_sc, and loop through each file and combine to combined_sc\n",
    "    # combined_sc will be a list of dictionaries\n",
    "    combined_sc = []\n",
    "    for file in os.listdir():\n",
    "        if(fnmatch.fnmatch(file, 'SP_FILING_SENTIMENT*')):\n",
    "            try:\n",
    "                with open(file,'r',encoding='utf_8') as sc:# gzip 用来打开压缩文件中的数据\n",
    "                    sc = ndjson.load(sc)\n",
    "                combined_sc.extend(sc)  # 加入最后\n",
    "            except Exception as e:\n",
    "                pass\n",
    "                flag+=1\n",
    "            continue\n",
    "\n",
    "\n",
    "    ## Filing Level Function\n",
    "    # Function for calculating the sentiment figures at the main filing level - i.e., the aggregated sentiment figures for the actual\n",
    "    # filing (EXCLUDING exhibits)\n",
    "    #Parameter: filing_type - the filing type of the data in combined_sc (examples: '10-k', '10-q')\n",
    "    def filing_level_func(filing_type):\n",
    "    # column names of sentiment scores package based on User Guide documentation\n",
    "        column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']\n",
    "    # create empty list called filin g_level_list\n",
    "        filing_level_list = [[]]\n",
    "    # loop through each element in combined_sc, check if the filing_type key exists in the 'SENTIMENT' key\n",
    "    # if it does append doc_level_list with ID and the sentiment scores\n",
    "    # if it does not, append doc_level_level with ID and NAs\n",
    "        for element in range(len(combined_sc)):\n",
    "            if combined_sc[element].get('SENTIMENT',{}).get(filing_type):\n",
    "                filing_level_list.append([str(combined_sc[element]['ID']), \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['avg_sent'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['sum_sent'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['hit_count'], \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['positive_hits'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['negative_hits'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['section_count'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type]['word_count']])\n",
    "            else:\n",
    "                filing_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "\n",
    "        # take contents of filing_level_list and put into a data frame called filing_level_df\n",
    "        filing_level_df = pd.DataFrame(filing_level_list[1:len(filing_level_list)], columns = column_names)    \n",
    "        \n",
    "        # convert the ID field to a string\n",
    "        filing_level_df['ID'] = filing_level_df['ID'].apply(str)\n",
    "        \n",
    "        # return the data frame\n",
    "        return filing_level_df\n",
    "\n",
    "\n",
    "    ## Section Level Function\n",
    "    # Function for calculating the sentiment figures at the section level\n",
    "    #Parameter: \n",
    "    # filing_type - the filing type of the data in combined_sc (examples: 'AR', 'QR', 'SR')\n",
    "    # section - the section of the filing_type of the data in combined_sc (examples: 'data', 'letter to shareholders', 'ceo report',etc)    \n",
    "    def section_level_func(filing_type, section):\n",
    "    # column names of sentiment scores package based on User Guide documentation\n",
    "        column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']   \n",
    "    # create empty list called section_level_list\n",
    "        section_level_list = [[]] \n",
    "    # loop through each element in combined_sc, check if the section key exists in the 'SENTIMENT' key -> filing_type key\n",
    "    # if it does append section_level_list with ID and the sentiment scores\n",
    "    # if it does not, append section_level_level with ID and NAs\n",
    "        for element in range(len(combined_sc)):\n",
    "            if combined_sc[element].get('SENTIMENT',{}).get(filing_type,{}).get(section):\n",
    "                section_level_list.append([str(combined_sc[element]['ID']), \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['avg_sent'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['sum_sent'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['hit_count'], \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['positive_hits'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['negative_hits'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['section_count'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section]['word_count']])\n",
    "            else:\n",
    "                section_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "\n",
    "        # take contents of section_level_list and put into a data frame called section_level_df\n",
    "        section_level_df = pd.DataFrame(section_level_list[1:len(section_level_list)], columns = column_names)    \n",
    "        \n",
    "        # convert the ID field to a string\n",
    "        section_level_df['ID'] = section_level_df['ID'].apply(str)\n",
    "        \n",
    "        # return the data frame\n",
    "        return section_level_df\n",
    "\n",
    "\n",
    "    ## Sub-section Level Function  \n",
    "    #Parameter: \n",
    "    # filing_type - the filing type of the data in combined_sc (examples: '10-k', '10-q')\n",
    "    # section - the section of the filing_type of the data in combined_sc (examples: 'data', 'letter to shareholders', 'ceo report',etc)    \n",
    "    # sub-section - the sub-section of the section of the filing_type in combined_sc (examples: 'data','esg','risk', etc.)  \n",
    "    def sub_level_func(filing_type, section, sub):\n",
    "    # column names of sentiment scores package based on User Guide documentation\n",
    "        column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']\n",
    "    # create empty list called sub_level_list\n",
    "        sub_level_list = [[]] \n",
    "    # loop through each element in combined_sc, check if the seub-section key exists in the 'SENTIMENT' key -> filing_type key -> section key\n",
    "    # if it does append sub_level_list with ID and the sentiment scores\n",
    "    # if it does not, append sub_level_list with ID and NAs\n",
    "        for element in range(len(combined_sc)):\n",
    "            if combined_sc[element].get('SENTIMENT',{}).get(filing_type,{}).get(section,{}).get(sub):\n",
    "                sub_level_list.append([str(combined_sc[element]['ID']), \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['avg_sent'], \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['sum_sent'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['hit_count'], \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['positive_hits'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['negative_hits'], \n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['section_count'],\n",
    "                            combined_sc[element]['SENTIMENT'][filing_type][section][sub]['word_count']])\n",
    "        \n",
    "            else: sub_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "            \n",
    "        # take contents of sub_level_list and put into a data frame called sub_level_df\n",
    "        sub_level_df = pd.DataFrame(sub_level_list[1:len(sub_level_list)], columns = column_names)    \n",
    "        \n",
    "        # convert the ID field to a string\n",
    "        sub_level_df['ID'] = sub_level_df['ID'].apply(str)\n",
    "        \n",
    "        # return the data frame\n",
    "        return sub_level_df\n",
    "\n",
    "    ## Examples of calling functions\n",
    "    # get the AR sentiment data at the filing level\n",
    "    ars = filing_level_func('ar')\n",
    "    rfs = section_level_func('ar','RF')\n",
    "    # rfs1 = sub_level_func('ar',section='data','data')\n",
    "\n",
    "    # print(rfs)\n",
    "    # Join reference information with sentiment metrics\n",
    "    combined = pd.merge(left = ref_normalize, right = ars, on = 'ID')\n",
    "    output= pd.merge(left = combined, right = rfs, on = 'ID')\n",
    "\n",
    "    print(flag)\n",
    "\n",
    "    # drop the duplicate \n",
    "    len_init=len(output)\n",
    "    print(len_init)\n",
    "    output.duplicated(['ID'])\n",
    "    output=output.drop_duplicates(['ID'])\n",
    "    len_out=len(output)\n",
    "    drop_len=len_init-len_out\n",
    "    print(drop_len)\n",
    "    # print(output)\n",
    "    output.to_csv(\"C:\\\\vscode\\\\code\\\\practicum\\\\outcome\\\\output_\"+i+\".csv\",sep=\",\",header=True)\n",
    "\n",
    "    # combined.groupy({})\n",
    "    # type(combined)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3e98d5f956c394c058448dea59b0542e2791614d79d89dd6218c183f95136fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
