{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ID DOCUMENT_ID DOCUMENT_TYPE FILING_DATE          MODIFIED_AT  \\\n",
      "0     5236338    13926002            AR  2009-04-14  2021-06-24 07:29:53   \n",
      "1     5236338    13926002            AR  2009-04-14  2021-06-24 07:29:53   \n",
      "2     5236338    13926002            AR  2009-04-14  2021-06-24 07:29:53   \n",
      "3     5236338    13926002            AR  2009-04-14  2021-06-24 07:29:53   \n",
      "4     5236338    13926002            AR  2009-04-14  2021-07-01 11:18:10   \n",
      "...       ...         ...           ...         ...                  ...   \n",
      "2368  9175111   197589149            AR  2022-12-30  2023-01-04 07:45:34   \n",
      "2369  9175111   197589149            AR  2022-12-30  2023-01-04 07:45:34   \n",
      "2370  9175111   197589149            AR  2022-12-30  2023-01-04 07:45:34   \n",
      "2371  9233806   198200435            AR  2023-01-18  2023-01-19 12:58:02   \n",
      "2372  9233978   198256029            AR  2023-01-18  2023-01-19 12:25:03   \n",
      "\n",
      "                               DETAIL_JSON.company_name  \\\n",
      "0                 Dapai International Holdings Co. Ltd.   \n",
      "1                 Dapai International Holdings Co. Ltd.   \n",
      "2                 Dapai International Holdings Co. Ltd.   \n",
      "3                 Dapai International Holdings Co. Ltd.   \n",
      "4                 Dapai International Holdings Co. Ltd.   \n",
      "...                                                 ...   \n",
      "2368                China Kepei Education Group Limited   \n",
      "2369                China Kepei Education Group Limited   \n",
      "2370                China Kepei Education Group Limited   \n",
      "2371                  China Putian Food Holding Limited   \n",
      "2372  Shanghai Fudan Microelectronics Group Company ...   \n",
      "\n",
      "     DETAIL_JSON.parsing_status  avg_sent_x  sum_sent_x  hit_count_x  ...  \\\n",
      "0                       SUCCESS    0.856044   21.401112         2366  ...   \n",
      "1                       SUCCESS    0.856044   21.401112         2366  ...   \n",
      "2                       SUCCESS    0.856044   21.401112         2366  ...   \n",
      "3                       SUCCESS    0.856044   21.401112         2366  ...   \n",
      "4                       SUCCESS    0.856044   21.401112         2366  ...   \n",
      "...                         ...         ...         ...          ...  ...   \n",
      "2368                    SUCCESS    0.334245   43.117591         2753  ...   \n",
      "2369                    SUCCESS    0.332548   42.898689         2741  ...   \n",
      "2370                    SUCCESS    0.332548   42.898689         2741  ...   \n",
      "2371                    SUCCESS    0.248676   25.862255         2113  ...   \n",
      "2372                    SUCCESS    0.452434   28.503323         2054  ...   \n",
      "\n",
      "      negative_hits_x  section_count_x  word_count_x  avg_sent_y  sum_sent_y  \\\n",
      "0                 672              129         29037         NaN         NaN   \n",
      "1                 672              129         29037         NaN         NaN   \n",
      "2                 672              129         29037         NaN         NaN   \n",
      "3                 672              129         29037         NaN         NaN   \n",
      "4                 672              129         29037         NaN         NaN   \n",
      "...               ...              ...           ...         ...         ...   \n",
      "2368              827              305         68151         0.0         0.0   \n",
      "2369              826              311         68247         0.0         0.0   \n",
      "2370              826              311         68247         0.0         0.0   \n",
      "2371              761              246         49601         NaN         NaN   \n",
      "2372              672              181         46728         NaN         NaN   \n",
      "\n",
      "      hit_count_y  positive_hits_y  negative_hits_y  section_count_y  \\\n",
      "0             NaN              NaN              NaN              NaN   \n",
      "1             NaN              NaN              NaN              NaN   \n",
      "2             NaN              NaN              NaN              NaN   \n",
      "3             NaN              NaN              NaN              NaN   \n",
      "4             NaN              NaN              NaN              NaN   \n",
      "...           ...              ...              ...              ...   \n",
      "2368          0.0              0.0              0.0              1.0   \n",
      "2369          0.0              0.0              0.0              1.0   \n",
      "2370          0.0              0.0              0.0              1.0   \n",
      "2371          NaN              NaN              NaN              NaN   \n",
      "2372          NaN              NaN              NaN              NaN   \n",
      "\n",
      "      word_count_y  \n",
      "0              NaN  \n",
      "1              NaN  \n",
      "2              NaN  \n",
      "3              NaN  \n",
      "4              NaN  \n",
      "...            ...  \n",
      "2368          57.0  \n",
      "2369          57.0  \n",
      "2370          57.0  \n",
      "2371           NaN  \n",
      "2372           NaN  \n",
      "\n",
      "[2373 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ndjson\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "\n",
    "# folder with the Reference, Data, Word Count, and Sentiment zipped files\n",
    "# os.chdir('C:\\\\vscode\\\\code\\\\practicum\\\\JSON_Data')\n",
    "os.chdir('E:\\\\data\\\\CN\\\\en-US')\n",
    "\n",
    "flag=0\n",
    "#### Filing Reference Package ####\n",
    "# create an empty list, combined_ref, and loop through each file and combine to combined_ref\n",
    "# combined ref will be a list of dictionaries\n",
    "combined_ref = []\n",
    "for file in os.listdir():\n",
    "    if(fnmatch.fnmatch(file, 'SP_FILING_REFERENCE*')):\n",
    "        with open(file,'r',encoding='utf_8') as ref:\n",
    "            ref = ndjson.load(ref)\n",
    "        combined_ref.extend(ref)    \n",
    " \n",
    "# normalize the reference package from its NDJSON form into a Data Frame - uses from pandas.io.json import json_normalize\n",
    "ref_normalize = pd.json_normalize(combined_ref)\n",
    "\n",
    "# change order of columns to reflect User Guide documentation\n",
    "ref_normalize = ref_normalize[['ID', 'DOCUMENT_ID','DOCUMENT_TYPE','FILING_DATE','MODIFIED_AT','DETAIL_JSON.company_name', 'DETAIL_JSON.parsing_status']]\n",
    "\n",
    "# convert the ID and document ID field to a string\n",
    "ref_normalize['ID'] = ref_normalize['ID'].apply(str)\n",
    "ref_normalize['DOCUMENT_ID'] = ref_normalize['DOCUMENT_ID'].apply(str)\n",
    "# ref_normalize['DETAIL_JSON.ISIN_active'] = ref_normalize['DETAIL_JSON.ISIN_active'].apply(str)\n",
    "# ref_normalize['DETAIL_JSON.SP_DocumentId'] = ref_normalize['DETAIL_JSON.SP_DocumentId'].apply(str)\n",
    "\n",
    "#### Sentiment Score Package ####\n",
    "# create an empty list, combined_sc, and loop through each file and combine to combined_sc\n",
    "# combined_sc will be a list of dictionaries\n",
    "combined_sc = []\n",
    "for file in os.listdir():\n",
    "    if(fnmatch.fnmatch(file, 'SP_FILING_SENTIMENT*')):\n",
    "        try:\n",
    "            with open(file,'r',encoding='utf_8') as sc:# gzip 用来打开压缩文件中的数据\n",
    "                sc = ndjson.load(sc)\n",
    "            combined_sc.extend(sc)  # 加入最后\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            flag+=1\n",
    "        continue\n",
    "\n",
    "\n",
    "## Filing Level Function\n",
    "# Function for calculating the sentiment figures at the main filing level - i.e., the aggregated sentiment figures for the actual\n",
    "# filing (EXCLUDING exhibits)\n",
    "#Parameter: filing_type - the filing type of the data in combined_sc (examples: '10-k', '10-q')\n",
    "def filing_level_func(filing_type):\n",
    "# column names of sentiment scores package based on User Guide documentation\n",
    "    column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']\n",
    "# create empty list called filin g_level_list\n",
    "    filing_level_list = [[]]\n",
    "# loop through each element in combined_sc, check if the filing_type key exists in the 'SENTIMENT' key\n",
    "# if it does append doc_level_list with ID and the sentiment scores\n",
    "# if it does not, append doc_level_level with ID and NAs\n",
    "    for element in range(len(combined_sc)):\n",
    "        if combined_sc[element].get('SENTIMENT',{}).get(filing_type):\n",
    "            filing_level_list.append([str(combined_sc[element]['ID']), \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['avg_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['sum_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['hit_count'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['positive_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['negative_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['section_count'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type]['word_count']])\n",
    "        else:\n",
    "            filing_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "\n",
    "    # take contents of filing_level_list and put into a data frame called filing_level_df\n",
    "    filing_level_df = pd.DataFrame(filing_level_list[1:len(filing_level_list)], columns = column_names)    \n",
    "     \n",
    "    # convert the ID field to a string\n",
    "    filing_level_df['ID'] = filing_level_df['ID'].apply(str)\n",
    "    \n",
    "    # return the data frame\n",
    "    return filing_level_df\n",
    "\n",
    "\n",
    "## Section Level Function\n",
    "# Function for calculating the sentiment figures at the section level\n",
    "#Parameter: \n",
    "# filing_type - the filing type of the data in combined_sc (examples: 'AR', 'QR', 'SR')\n",
    "# section - the section of the filing_type of the data in combined_sc (examples: 'data', 'letter to shareholders', 'ceo report',etc)    \n",
    "def section_level_func(filing_type, section):\n",
    "# column names of sentiment scores package based on User Guide documentation\n",
    "    column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']   \n",
    "# create empty list called section_level_list\n",
    "    section_level_list = [[]] \n",
    "# loop through each element in combined_sc, check if the section key exists in the 'SENTIMENT' key -> filing_type key\n",
    "# if it does append section_level_list with ID and the sentiment scores\n",
    "# if it does not, append section_level_level with ID and NAs\n",
    "    for element in range(len(combined_sc)):\n",
    "        if combined_sc[element].get('SENTIMENT',{}).get(filing_type,{}).get(section):\n",
    "            section_level_list.append([str(combined_sc[element]['ID']), \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['avg_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['sum_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['hit_count'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['positive_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['negative_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['section_count'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section]['word_count']])\n",
    "        else:\n",
    "            section_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "\n",
    "    # take contents of section_level_list and put into a data frame called section_level_df\n",
    "    section_level_df = pd.DataFrame(section_level_list[1:len(section_level_list)], columns = column_names)    \n",
    "    \n",
    "    # convert the ID field to a string\n",
    "    section_level_df['ID'] = section_level_df['ID'].apply(str)\n",
    "    \n",
    "    # return the data frame\n",
    "    return section_level_df\n",
    "\n",
    "\n",
    "## Sub-section Level Function  \n",
    "#Parameter: \n",
    "# filing_type - the filing type of the data in combined_sc (examples: '10-k', '10-q')\n",
    "# section - the section of the filing_type of the data in combined_sc (examples: 'data', 'letter to shareholders', 'ceo report',etc)    \n",
    "# sub-section - the sub-section of the section of the filing_type in combined_sc (examples: 'data','esg','risk', etc.)  \n",
    "def sub_level_func(filing_type, section, sub):\n",
    "# column names of sentiment scores package based on User Guide documentation\n",
    "    column_names = ['ID','avg_sent','sum_sent','hit_count','positive_hits','negative_hits','section_count','word_count']\n",
    "# create empty list called sub_level_list\n",
    "    sub_level_list = [[]] \n",
    "# loop through each element in combined_sc, check if the seub-section key exists in the 'SENTIMENT' key -> filing_type key -> section key\n",
    "# if it does append sub_level_list with ID and the sentiment scores\n",
    "# if it does not, append sub_level_list with ID and NAs\n",
    "    for element in range(len(combined_sc)):\n",
    "        if combined_sc[element].get('SENTIMENT',{}).get(filing_type,{}).get(section,{}).get(sub):\n",
    "            sub_level_list.append([str(combined_sc[element]['ID']), \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['avg_sent'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['sum_sent'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['hit_count'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['positive_hits'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['negative_hits'], \n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['section_count'],\n",
    "                          combined_sc[element]['SENTIMENT'][filing_type][section][sub]['word_count']])\n",
    "      \n",
    "        else: sub_level_list.append([str(combined_sc[element]['ID']), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,np.nan])\n",
    "        \n",
    "    # take contents of sub_level_list and put into a data frame called sub_level_df\n",
    "    sub_level_df = pd.DataFrame(sub_level_list[1:len(sub_level_list)], columns = column_names)    \n",
    "    \n",
    "    # convert the ID field to a string\n",
    "    sub_level_df['ID'] = sub_level_df['ID'].apply(str)\n",
    "    \n",
    "    # return the data frame\n",
    "    return sub_level_df\n",
    "\n",
    "## Examples of calling functions\n",
    "# get the AR sentiment data at the filing level\n",
    "ars = filing_level_func('ar')\n",
    "rfs = section_level_func('ar','RF')\n",
    "# rfs1 = sub_level_func('ar',section='data','data')\n",
    "\n",
    "# print(rfs)\n",
    "# Join reference information with sentiment metrics\n",
    "combined = pd.merge(left = ref_normalize, right = ars, on = 'ID')\n",
    "output= pd.merge(left = combined, right = rfs, on = 'ID')\n",
    "\n",
    "\n",
    "print(output)\n",
    "# output.to_csv(\"C:\\\\vscode\\\\code\\\\practicum\\\\outcome\\\\output.csv\",sep=\",\",header=True)\n",
    "\n",
    "# combined.groupy({})\n",
    "# type(combined)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3e98d5f956c394c058448dea59b0542e2791614d79d89dd6218c183f95136fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
